 # Developing a statistical language model of Turkish that will use N-grams of Turkish syllables
 
 Separated each word into its syllables
 
 Calculated the 1-Gram, 2-Gram, 3-Gram, 4-Gram and 5-Gram tables for this set using 95% of the set. Used smoothing, which is GT smoothing.
 
 Calculated perplexity of the 1-Gram to 5-Gram models using the chain rule with the Markov assumption for each sentence. Used the remaining 5% of the set for these calculations. Made a table of findings in report and explained results.
 
 Produced random sentences for each N-Gram model. Picked one of the best 5 letters randomly. Included these random sentences in report and discussed the produced sentences.
 
